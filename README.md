# TFFLM
Motivation: Transcription factors (TFs) are proteins that bind to specific DNA sequences to regu-late the expression of the genome. The identification of transcription factor binding sites (TFBSs) is ex-tremely important for understanding the process of gene expression. On the one hand, classic deep learning methods for TFBSs prediction usually fail to capture the dependencies between genomic se-quences since their commonly used one-hot codes are mutually orthogonal. On the other hand, these methods usually perform poorly when samples are inadequate.
Results: To address these two challenges, we developed a novel language model for mining TFBSs using human genomic data and ChIP-seq datasets, named Transcription Factors Fine-tune Language Model (TFFLM). It can capture the dependencies between genome sequences based on the context of human genomic data and then fine-tune the features of TFBSs tasks using different ChIP-seq datasets. First, we compared TFFLM with the existing widely used methods on 69 datasets and we achieved the state-of-the-art performance. Moreover, we conducted comparative experiments on complex TFs such as POLR2A and small data set such as KMD5A and the results show that TFFLM still achieved a signif-icant improvement. Finally, through visualization analysis of one-hot encoding and TFFLM, we found that one-hot encoding completely cut off the dependencies of DNA sequences themselves, while TFFLM using language models can well represent the dependency of DNA sequence.

** 1. Environment setup
We recommend you to build a python virtual environment with Anaconda. Also, please make sure you have at least one NVIDIA GPU with Linux x86_64 Driver Version <= 384.145 (compatible with CUDA 9.0). 
